[2025-12-01 03:42:49,756][__main__][INFO] - ====== Configuration ======
[2025-12-01 03:42:49,772][__main__][INFO] - 
train:
  seed: 42
  device: cuda
  epochs: 5
checkpoint:
  enabled: true
  save_dir: checkpoints
model:
  name: seq2seq_transformer
  wrapper:
    name: Seq2SeqWrapper
  shared:
    src_vocab_size: 21128
    tgt_vocab_size: 21128
    d_model: 128
    pe_cfg:
      name: SinusoidalPE
      d_model: 128
      dropout: 0.1
      max_len: 5000
  arch:
    encoder:
      name: TransformerEncoder
      input_dim: 21128
      d_model: 128
      nhead: 2
      num_layers: 2
      pe_cfg:
        name: SinusoidalPE
        d_model: 128
        dropout: 0.1
        max_len: 5000
    decoder:
      name: TransformerDecoder
      output_dim: 21128
      d_model: 128
      nhead: 2
      num_layers: 2
      pe_cfg:
        name: SinusoidalPE
        d_model: 128
        dropout: 0.1
        max_len: 5000
dataset:
  name: Opus100Dataset
  train_path: /data1/jinyu_wang/projects/transformer-template/data/datasets/opus-100/train/train-00000-of-00001.parquet
  test_path: /data1/jinyu_wang/projects/transformer-template/data/datasets/opus-100/test/test-00000-of-00001.parquet
  source_lang: en
  target_lang: zh
  tokenizer_path: /data1/jinyu_wang/projects/transformer-template/data/pretrain_models/bert-base-chinese
  max_samples: 5000
  seq_len: 64
  batch_size: 32
  num_workers: 0
optimizer:
  name: AdamW
  params:
    lr: 0.0005
    weight_decay: 0.01
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
criterion:
  name: CrossEntropyLoss
  params: {}
logger:
  enable: false
  project: hydra-styple-transformer
  entity: null
  name: null
  group: null
  tags: []
  notes: null
  mode: online
scheduler:
  name: CosineAnnealingLR
  params:
    T_max: 5
    eta_min: 1.0e-06

[2025-12-01 03:42:49,775][__main__][INFO] - ===========================
[2025-12-01 03:42:49,775][__main__][INFO] - Initializing Solver with GPU: 6
[2025-12-01 03:42:49,821][src.data.datasets][INFO] - [Data] Loading local parquet file: /data1/jinyu_wang/projects/transformer-template/data/datasets/opus-100/train/train-00000-of-00001.parquet...
[2025-12-01 03:42:52,112][src.data.datasets][INFO] - [Data] Selecting first 5000 samples...
[2025-12-01 03:42:52,114][src.data.datasets][INFO] - [Data] Loading Tokenizer from: /data1/jinyu_wang/projects/transformer-template/data/pretrain_models/bert-base-chinese...
[2025-12-01 03:42:52,160][src.solver][INFO] - [Solver] Auto-setting vocab size to: 21128
[2025-12-01 03:42:52,167][src.models.builders][INFO] - [Builder] Instantiating Encoder: TransformerEncoder
[2025-12-01 03:42:52,682][src.models.builders][INFO] - [Builder] Instantiating Decoder: TransformerDecoder
[2025-12-01 03:42:53,201][src.models.builders][INFO] - [Builder] Instantiating Model: Seq2SeqWrapper
[2025-12-01 03:42:55,827][src.solver][INFO] - [Solver] Checkpoints will be saved to: /data1/jinyu_wang/projects/pytorch-hydra/outputs/2025-12-01/03-42-49/checkpoints
[2025-12-01 03:42:55,828][src.solver][INFO] - Start Training Process... Total Epochs: 5
