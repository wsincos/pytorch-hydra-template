train:
  seed: 42
  device: cuda
  epochs: 3
checkpoint:
  enabled: true
  save_dir: checkpoints
model:
  name: seq2seq_transformer
  wrapper:
    name: Seq2SeqWrapper
  shared:
    src_vocab_size: 21128
    tgt_vocab_size: 21128
    d_model: 512
    pe_cfg:
      name: SinusoidalPE
      d_model: ${model.shared.d_model}
      dropout: 0.1
      max_len: 5000
  arch:
    encoder:
      name: TransformerEncoder
      input_dim: ${model.shared.src_vocab_size}
      d_model: ${model.shared.d_model}
      nhead: 8
      num_layers: 6
      pe_cfg: ${model.shared.pe_cfg}
    decoder:
      name: TransformerDecoder
      output_dim: ${model.shared.tgt_vocab_size}
      d_model: ${model.shared.d_model}
      nhead: 8
      num_layers: 6
      pe_cfg: ${model.shared.pe_cfg}
dataset:
  name: Opus100Dataset
  train_path: ./data/datasets/opus-100/train/train-00000-of-00001.parquet
  test_path: ./data/datasets/opus-100/test/test-00000-of-00001.parquet
  source_lang: en
  target_lang: zh
  tokenizer_path: ./data/pretrained_models/bert-base-multilingual-cased
  max_samples: 500
  seq_len: 64
  batch_size: 32
  num_workers: 8
optimizer:
  name: AdamW
  params:
    lr: 0.0005
    weight_decay: 0.01
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
criterion:
  name: CrossEntropyLoss
  params: {}
logger:
  enable: true
  project: hydra-style-transformer
  entity: null
  name: test_run_03
  group: null
  tags:
  - opus
  - transformer
  notes: null
  mode: online
callback:
  ckpt:
    name: CheckpointCallback
    params:
      save_every: 1
      keep_last: true
  early_stop:
    name: EarlyStopping
    params:
      patience: 3
      min_delta: 0.001
  training_monitor:
    name: TrainingMonitor
    params:
      log_every_n_steps: 50
  translation_monitor:
    name: TranslationMonitor
    params:
      num_samples: 10
scheduler:
  name: CosineAnnealingLR
  params:
    T_max: ${train.epochs}
    eta_min: 1.0e-06
