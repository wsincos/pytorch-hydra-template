# 对应 src/model/builders.py 里的判断逻辑
name: seq2seq_transformer

wrapper:
  name: Seq2SeqWrapper

shared:
  # 这里的vocab_size只是为了占位，实际会被Tokenizer中真实词表覆盖
  src_vocab_size: 21128
  tgt_vocab_size: 21128
  d_model: 512  # 标准宽度。128 太窄，表达能力不够；512 是平衡点。
  pe_cfg:
    name: SinusoidalPE
    d_model: ${model.shared.d_model}
    dropout: 0.1
    max_len: 5000

arch:
  encoder:
    name: TransformerEncoder
    input_dim: ${model.shared.src_vocab_size}
    d_model: ${model.shared.d_model}
    dim_feedforward: 
    nhead: 8
    num_layers: 6
    pe_cfg: ${model.shared.pe_cfg}

  
  decoder:
    name: TransformerDecoder
    output_dim: ${model.shared.tgt_vocab_size}
    d_model: ${model.shared.d_model}
    dim_feedforward: 
    nhead: 8
    num_layers: 6
    pe_cfg: ${model.shared.pe_cfg}
